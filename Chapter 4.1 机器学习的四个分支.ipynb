{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习只是冰山一角——机器学习是非常宽泛的领域，其子领域的划分非常复杂。机器学习算法大致可分为四大类:  \n",
    "    1. 监督学习  \n",
    "    2. 无监督学习  \n",
    "    3. 自监督学习  \n",
    "    4. 强化学习  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1　监督学习\n",
    "虽然监督学习主要包括分类和回归，但还有更多的奇特变体，主要包括如下几种。\n",
    "\n",
    "序列生成（sequence generation）。给定一张图像，预测描述图像的文字。序列生成有时可以被重新表示为一系列分类问题，比如反复预测序列中的单词或标记。  \n",
    "\n",
    "语法树预测（syntax tree prediction）。给定一个句子，预测其分解生成的语法树。  \n",
    "\n",
    "目标检测（object detection）。给定一张图像，在图中特定目标的周围画一个边界框。这个问题也可以表示为分类问题（给定多个候选边界框，对每个框内的目标进行分类）或分类与回归联合问题（用向量回归来预测边界框的坐标）。  \n",
    "\n",
    "图像分割（image segmentation）。给定一张图像，在特定物体上画一个像素级的掩模（mask）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2　无监督学习\n",
    "无监督学习是指在没有目标的情况下寻找输入数据的有趣变换，其目的在于数据可视化、数据压缩、数据去噪或更好地理解数据中的相关性。无监督学习是数据分析的必备技能，在解决监督学习问题之前，为了更好地了解数据集，它通常是一个必要步骤。降维（dimensionality reduction）和聚类（clustering）都是众所周知的无监督学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3　自监督学习\n",
    "自监督学习是监督学习的一个特例，它与众不同，值得单独归为一类。自监督学习是没有人工标注的标签的监督学习，你可以将它看作没有人类参与的监督学习。标签仍然存在（因为总要有什么东西来监督学习过程），但它们是从输入数据中生成的，通常是使用启发式算法生成的。\n",
    "\n",
    "举个例子，自编码器（autoencoder）是有名的自监督学习的例子，其生成的目标就是未经修改的输入。同样，给定视频中过去的帧来预测下一帧，或者给定文本中前面的词来预测下一个词，都是自监督学习的例子［这两个例子也属于时序监督学习（temporally supervised learning），即用未来的输入数据作为监督］。注意，监督学习、自监督学习和无监督学习之间的区别有时很模糊，这三个类别更像是没有明确界限的连续体。自监督学习可以被重新解释为监督学习或无监督学习，这取决于你关注的是学习机制还是应用场景。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4　强化学习\n",
    "强化学习一直以来被人们所忽视，但最近随着  Google的  DeepMind公司将其成功应用于学习玩 Atari游戏（以及后来学习下围棋并达到最高水平），机器学习的这一分支开始受到大量关注。在强化学习中，智能体（agent）接收有关其环境的信息，并学会选择使某种奖励最大化的行动。例如，神经网络会“观察”视频游戏的屏幕并输出游戏操作，目的是尽可能得高分，这种神经网络可以通过强化学习来训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 分类和回归术语表\n",
    "\n",
    "分类和回归都包含很多专业术语。前面你已经见过一些术语，在后续章节会遇到更多。这些术语在机器学习领域都有确切的定义，你应该了解这些定义。\n",
    "\n",
    "样本（sample）或输入（input）：进入模型的数据点。  \n",
    "预测（prediction）或输出（output）：从模型出来的结果。  \n",
    "目标（target）：真实值。对于外部数据源，理想情况下，模型应该能够预测出目标。  \n",
    "预测误差（prediction  error）或损失值（loss value）：模型预测与目标之间的距离。  \n",
    "类别（class）：分类问题中供选择的一组标签。例如，对猫狗图像进行分类时，“狗”和“猫”就是两个类别。  \n",
    "标签（label）：分类问题中类别标注的具体例子。比如，如果   1234号图像被标注为包含类别“狗”，那么“狗”就是 1234号图像的标签。  \n",
    "真值（ground-truth）或标注（annotation）：数据集的所有目标，通常由人工收集。  \n",
    "二分类（binary   classification）：一种分类任务，每个输入样本都应被划分到两个互斥的类别中。  \n",
    "多分类（multiclass   classification）：一种分类任务，每个输入样本都应被划分到两个以上的类别中，比如手写数字分类。  \n",
    "多标签分类（multilabel   classification）：一种分类任务，每个输入样本都可以分配多个标签。举个例子，如果一幅图像里可能既有猫又有狗，那么应该同时标注“猫”标签和“狗”标签。每幅图像的标签个数通常是可变的。  \n",
    "标量回归（scalar   regression）：目标是连续标量值的任务。预测房价就是一个很好的例子，不同的目标价格形成一个连续的空间。  \n",
    "向量回归（vector   regression）：目标是一组连续值（比如一个连续向量）的任务。如果对多个值（比如图像边界框的坐标）进行回归，那就是向量回归。  \n",
    "小批量（mini-batch）或批量（batch）：模型同时处理的一小部分样本（样本数通常为 8~128）。样本数通常取 2的幂，这样便于   GPU上的内存分配。训练时，小批量用来为模型权重计算一次梯度下降更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
